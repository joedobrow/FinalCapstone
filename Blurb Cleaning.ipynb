{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file take the blurbs scraped from a major book website and cleans them. Process is as follows:\n",
    "1. Cleaning the URLs of the blurbs that were scraped so we can compare it with the 'title' column in our book-crossings dataset\n",
    "2. Using a function that crawls through the URLs and matches them to titles, shifting the cells up and down throughout the process. This is because the scrape skipped many titles and added nonsense data randomly.\n",
    "3. Many of the blurbs have repeated sections that I identified and deleted.\n",
    "4. For each blurb, break into sentences, generate BERT vectors using BERT-as-a-service\n",
    "5. Add vectors into the books dataframe and save as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n"
     ]
    }
   ],
   "source": [
    "### Data\n",
    "\n",
    "# BookCrossing\n",
    "# http://www2.informatik.uni-freiburg.de/~cziegler/BX/\n",
    "\n",
    "df_ratings = pd.read_csv('/DataScience/BX-CSV-Dump/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding = \"latin-1\")\n",
    "df_users = pd.read_csv('/DataScience/BX-CSV-Dump/BX-Users.csv', sep=';', encoding='latin-1')\n",
    "df_books = pd.read_csv('/DataScience/BX-CSV-Dump/BX-Books.csv', sep=';', error_bad_lines=False, encoding = \"latin-1\")\n",
    "\n",
    "# Renaming columns for ease of use, and dropping image links I wont be using\n",
    "df_ratings.rename(columns={'User-ID': 'User', 'Book-Rating': 'Rating'}, inplace=True)\n",
    "df_users.rename(columns={'User-ID': 'User'}, inplace=True)\n",
    "df_books.rename(columns={'ISBN': 'ISBN', 'Book-Title': 'Title', 'Year-Of-Publication': 'Year', 'Book-Author': 'Author'}, inplace=True)\n",
    "df_books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], 1, inplace=True)\n",
    "\n",
    "# Using only explicit ratings. 0 ratings are 'implicit' in documentation of data:\n",
    "df_ratings = df_ratings[df_ratings['Rating'] > 0]\n",
    "\n",
    "# Inner join df_ratings and df_books\n",
    "df_ratings = df_ratings[df_ratings['ISBN'].isin(df_books.ISBN.unique())]\n",
    "df_books = df_books[df_books['ISBN'].isin(df_ratings.ISBN.unique())]\n",
    "\n",
    "df_books.reset_index(inplace=True, drop=True)\n",
    "df_ratings.reset_index(inplace=True, drop=True)\n",
    "df_users.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# 2 batches of scraped blurbs from Goodreads\n",
    "first_scrape = pd.read_csv('/Users/jdobrow/Code/blurbs1.csv')\n",
    "second_scrape = pd.read_csv('/Users/jdobrow/Code/blurbs2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to add our scraped blurbs to df_books, but the scraped data is messy and\n",
    "# does not line up correctly.\n",
    "\n",
    "# Our titles from the scraped URLs. They almost, but not quite match the actual titles.\n",
    "\n",
    "scrapes = [first_scrape, second_scrape]\n",
    "url_titles = []\n",
    "for chunk in scrapes:\n",
    "    for book in range(len(chunk)):\n",
    "        skip, url_split = 0, -1\n",
    "        for i in range(len(chunk.URL[book])):\n",
    "            skip += 1 if chunk.URL[book][i] == '.':\n",
    "                if skip == 3:\n",
    "                    url_split = chunk.URL[book][i+1:].lower()\n",
    "                    url_split = re.sub(r'[^\\w\\s]',' ',url_split).split('_')\n",
    "        if url_split == -1:\n",
    "            for i in range(len(chunk.URL[book])):\n",
    "                if chunk.URL[book][i] == '-':\n",
    "                    url_split = chunk.URL[book][i+1:].lower().split('-')\n",
    "                    break\n",
    "        url_titles.append(url_split)\n",
    "\n",
    "    # The actual text of the blurb\n",
    "    blurbs = list(chunk.text)\n",
    "\n",
    "    # The titles from our data, tidied up a bit to assist matching\n",
    "    if len(chunk) == len(second_scrape):\n",
    "        book_titles = list(df_books['Title'][100000:])\n",
    "        for i in range(len(df_books) - len(chunk) - 100000):\n",
    "            url_titles.append(None)\n",
    "            blurbs.append(None)\n",
    "    else:\n",
    "        book_titles = list(df_books['Title'][:100000])\n",
    "        for i in range(len(100000 - len(chunk)):\n",
    "            url_titles.append(None)\n",
    "            blurbs.append(None)\n",
    "                       \n",
    "    for title in range(len(book_titles)):\n",
    "        book_titles[title] = book_titles[title].lower()\n",
    "        book_titles[title] = re.sub(r'[^\\w\\s]',' ', book_titles[title]).split()\n",
    "\n",
    "    \n",
    "compare_df = pd.DataFrame()\n",
    "compare_df['URLTitle'] = url_titles\n",
    "compare_df['BookTitle'] = book_titles\n",
    "compare_df['Blurb'] = blurbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the title and url match each other. Matches are determined by how many\n",
    "# shared words there are\n",
    "stop_words = ['the', 'of', 'if', 'and', 'it', 'as', 'or']\n",
    "def check_matched_up(url_index, book_index, data):\n",
    "    try:\n",
    "        count = 0\n",
    "        for k in data.URLTitle[url_index]:\n",
    "            if k in data.BookTitle[book_index]:\n",
    "                if k not in stop_words:\n",
    "                    count += 1\n",
    "        if count >= max(min(len(data.URLTitle[url_index])//2, len(data.BookTitle[book_index])//2), 1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "   \n",
    "    except:\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterates through the data and check if titles match up, and if not corrects them by\n",
    "# shifting all of the data up or down or in some cases swapping entries.\n",
    "goto = len(compare_df)\n",
    "beginning = 0\n",
    "while beginning < goto:\n",
    "\n",
    "    for row in range(beginning, goto):\n",
    "        if (check_matched_up(row, row, compare_df) == True):\n",
    "            # everything good\n",
    "            #print(row, 'case 1')\n",
    "            None\n",
    "        elif (check_matched_up(row, row + 1, compare_df) == True) and (check_matched_up(row + 1, row, compare_df) == True):\n",
    "            # 2 adjacent rows need to be swapped\n",
    "            url1, url2 = compare_df.URLTitle[row], compare_df.URLTitle[row + 1]\n",
    "            blurb1, blurb2 = compare_df.Blurb[row], compare_df.Blurb[row + 1]\n",
    "            compare_df.iloc[row, 0], compare_df.iloc[row + 1, 0] = url2, url1\n",
    "            compare_df.iloc[row, 2], compare_df.iloc[row + 1, 2] = blurb2, blurb1\n",
    "            #print(row, 'case 2')\n",
    "            break\n",
    "        elif (check_matched_up(row + 1, row, compare_df) == True):\n",
    "            # Insert an empty entry into book titles to match things up\n",
    "            book_titles.insert(row, None)\n",
    "            url_titles.append(None)\n",
    "            blurbs.append(None)\n",
    "            compare_df = pd.DataFrame()\n",
    "            compare_df['URLTitle'] = url_titles\n",
    "            compare_df['BookTitle'] = book_titles\n",
    "            compare_df['Blurb'] = blurbs\n",
    "            #print(row, 'case 3')\n",
    "            break\n",
    "        elif (check_matched_up(row, row + 1, compare_df) == True):\n",
    "            # Insert an empty entry into url titles to match things up\n",
    "            url_titles.insert(row, None)\n",
    "            blurbs.insert(row, None)\n",
    "            book_titles.append(None)\n",
    "            compare_df = pd.DataFrame()\n",
    "            compare_df['URLTitle'] = url_titles\n",
    "            compare_df['BookTitle'] = book_titles\n",
    "            compare_df['Blurb'] = blurbs\n",
    "            #print(row, 'case 4')\n",
    "            break\n",
    "        else:\n",
    "            book_titles[row] = None\n",
    "            url_titles[row] = None\n",
    "            compare_df = pd.DataFrame()\n",
    "            compare_df['URLTitle'] = url_titles\n",
    "            compare_df['BookTitle'] = book_titles\n",
    "            compare_df['Blurb'] = blurbs\n",
    "            #print(row, 'case 5')\n",
    "    \n",
    "    beginning = row + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing rows\n",
    "compare_df = compare_df.dropna()\n",
    "compare_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move blurbs from compare_df to a new dataframe to work out of\n",
    "\n",
    "active = df_books.dropna().reset_index(drop=True)\n",
    "blurbs = []\n",
    "blurb_row = 0\n",
    "for book_row in range(len(active)):\n",
    "    cleaned_title = active['Title'][book_row].lower()\n",
    "    cleaned_title = re.sub(r'[^\\w\\s]',' ', cleaned_title).split()\n",
    "    if cleaned_title == compare_df.BookTitle[blurb_row]:\n",
    "        blurbs.append(compare_df.Blurb[blurb_row])\n",
    "        if blurb_row < len(compare_df):\n",
    "            blurb_row += 1\n",
    "    else:\n",
    "        blurbs.append(None)\n",
    "active['Blurb'] = blurbs\n",
    "\n",
    "active.dropna(inplace=True)\n",
    "active.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the nature of the scraped data, many of the blurbs have a repeated section.\n",
    "# This block finds the repeat and deletes it.\n",
    "\n",
    "updated_blurbs = []\n",
    "\n",
    "for row in range(len(active)):\n",
    "    # Most of the very short blurbs appear to be nonsense\n",
    "    if len(active.Blurb[row]) > 100:\n",
    "        try:\n",
    "            # A lot of random case by case errors are happening so I catch all with a\n",
    "            # try, and when it fails just go with the raw blurb since it's close enough.\n",
    "            go = False\n",
    "            string = active.Blurb[row][200:]\n",
    "            last_i = -1\n",
    "            first_i = 0\n",
    "            # the regular expression search doesn't like non alphanumeric characters so this\n",
    "            # searches for a chunk to find a repeating section.\n",
    "            while (go == False) and (first_i < 190):\n",
    "                first_char = '!'\n",
    "                first_i = last_i + 1\n",
    "                while not first_char.isalpha():\n",
    "                    first_char = active.Blurb[row][first_i]\n",
    "                    first_i += 1\n",
    "                last_char = 'a'\n",
    "                last_i = first_i + 1\n",
    "                str_len = 0\n",
    "                while (last_char.isalpha() or last_char.isspace()) and (str_len < 10):\n",
    "                    str_len = last_i - first_i + 1\n",
    "                    last_char = active.Blurb[row][last_i]\n",
    "                    last_i += 1\n",
    "                if str_len == 10:\n",
    "                    go = True\n",
    "                if last_i > len(active.Blurb[row]) - 2:\n",
    "                    go = True\n",
    "            a = re.search(r'{}'.format(active.Blurb[row][(first_i-1):(last_i-1)]), string)\n",
    "            \n",
    "            updated_blurbs.append(active.Blurb[row][a.start() + 200 - first_i + 1:])\n",
    "            \n",
    "        except:\n",
    "            updated_blurbs.append(active.Blurb[row])\n",
    "    else:\n",
    "        updated_blurbs.append(None)\n",
    "        \n",
    "active['Blurb'] = updated_blurbs\n",
    "active = active.dropna()\n",
    "active.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "501\n",
      "1001\n",
      "1501\n",
      "2001\n",
      "2501\n",
      "3001\n",
      "3501\n",
      "4001\n",
      "4501\n",
      "5001\n",
      "5501\n",
      "6001\n",
      "6501\n",
      "7001\n",
      "7501\n",
      "8001\n",
      "8501\n",
      "9001\n",
      "9501\n",
      "10001\n",
      "10501\n",
      "11001\n",
      "11501\n",
      "12001\n",
      "12501\n",
      "13001\n",
      "13501\n",
      "14001\n",
      "14501\n",
      "15001\n",
      "15501\n",
      "16001\n",
      "16501\n",
      "17001\n",
      "17501\n",
      "18001\n",
      "18501\n",
      "19001\n",
      "19501\n",
      "20001\n",
      "20501\n",
      "21001\n",
      "21501\n"
     ]
    }
   ],
   "source": [
    "# Get the BERT vectors for each sentence in a blurb, and then average them\n",
    "\n",
    "bc = BertClient()\n",
    "\n",
    "vector_list = []\n",
    "for blurb in active.Blurb:\n",
    "    raw = re.split('\\. |! |\\? ', blurb)\n",
    "    sentences = []\n",
    "    for sentence in raw:\n",
    "        if (len(sentence) > 0) and not sentence.isspace():\n",
    "            sentences.append(sentence)\n",
    "    vectors = bc.encode(sentences)\n",
    "    \n",
    "    mean_vector = vectors[0]\n",
    "    for i in range(len(vectors) - 1):\n",
    "        mean_vector = mean_vector + vectors[i + 1]\n",
    "    mean_vector = mean_vector/len(vectors)\n",
    "    vector_list.append(mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine books with vectors and save to a local csv file\n",
    "\n",
    "vectors = pd.DataFrame(vector_list)\n",
    "active = pd.concat([active, vectors], axis=1)\n",
    "active.to_csv('/DataScience/Final Capstone Files/books_with_blurbs_and_BERT_combined.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
